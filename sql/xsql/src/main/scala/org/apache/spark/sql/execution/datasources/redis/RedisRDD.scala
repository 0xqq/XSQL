/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.spark.sql.execution.datasources.redis

import java.util

import scala.collection.JavaConverters._

import redis.clients.jedis._
import redis.clients.util.JedisClusterCRC16

import org.apache.spark._
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.Row
import org.apache.spark.sql.types._
import org.apache.spark.sql.xsql.manager.RedisManager._

class RedisRDD(
    sc: SparkContext,
    val relation: RedisRelation,
    val filters: Map[String, Any],
    val requiredColumns: Array[String],
    val partitionNum: Int = 1)
  extends RDD[Row](sc, Seq.empty)
  with RedisTableScanTrait {
  override var key: Option[Any] = _
  override var range: Option[(Long, Long)] = _
  override var score: Option[(Double, Double)] = _
  analyzeFilter()

  override protected def getPreferredLocations(split: Partition): Seq[String] = {
    Seq(split.asInstanceOf[RedisPartition].redisConfig.initialAddr)
  }

  /**
   * hosts(ip:String, port:Int, startSlot:Int, endSlot:Int) are generated by the redis-cluster's
   * hash tragedy and partitionNum to divide the cluster to partitionNum
   *
   * @return hosts
   */
  private def scaleHostsWithPartitionNum(): Seq[(String, Int, Int, Int)] = {
    def split(host: RedisNode, cnt: Int) = {
      val endpoint = host.endpoint
      val start = host.startSlot
      val end = host.endSlot
      val range = (end - start) / cnt
      (0 until cnt).map(i => {
        (
          endpoint.host,
          endpoint.port,
          if (i == 0) start else (start + range * i + 1),
          if (i != cnt - 1) (start + range * (i + 1)) else end)
      })
    }

    val hosts = relation.redisConfig.hosts.sortBy(_.startSlot)

    if (hosts.size == partitionNum) {
      hosts.map(x => (x.endpoint.host, x.endpoint.port, x.startSlot, x.endSlot))
    } else if (hosts.size < partitionNum) {
      val presExtCnt = partitionNum / hosts.size
      val lastExtCnt = if (presExtCnt * hosts.size < partitionNum) {
        (presExtCnt + (partitionNum - presExtCnt * hosts.size))
      } else presExtCnt
      hosts.zipWithIndex.flatMap {
        case (host, idx) =>
          split(host, if (idx == hosts.size - 1) lastExtCnt else presExtCnt)
      }
    } else {
      val presExtCnt = hosts.size / partitionNum
      (0 until partitionNum).map { idx =>
        {
          val ip = hosts(idx * presExtCnt).endpoint.host
          val port = hosts(idx * presExtCnt).endpoint.port
          val start = hosts(idx * presExtCnt).startSlot
          val end = hosts(if (idx == partitionNum - 1) {
            (hosts.size - 1)
          } else {
            ((idx + 1) * presExtCnt - 1)
          }).endSlot
          (ip, port, start, end)
        }
      }
    }
  }

  override protected def getPartitions: Array[Partition] = {
    val hosts = scaleHostsWithPartitionNum()
    (0 until partitionNum)
      .map(i => {
        new RedisPartition(i, relation.redisConfig, (hosts(i)._3, hosts(i)._4))
      })
      .toArray
  }

  override def getRowkeys(sPos: Int, ePos: Int): Seq[String] = {
    if (key.isDefined) {
      val keys = if (key.get.isInstanceOf[Seq[String]]) {
        key.get.asInstanceOf[Seq[String]].map(_.toString)
      } else {
        Array(key.get.toString).toSeq
      }
      keys.filter(key => {
        val slot = JedisClusterCRC16.getSlot(key)
        slot >= sPos && slot <= ePos
      })
    } else {
      getKeys(sPos, ePos, getKeyPattern).asScala.toSeq
    }
  }

  override def compute(split: Partition, context: TaskContext): Iterator[Row] = {
    val partition: RedisPartition = split.asInstanceOf[RedisPartition]
    val rowkeys = getRowkeys(partition.slots._1, partition.slots._2)
    groupKeysByNode(relation.redisConfig.hosts, rowkeys.iterator)
      .flatMap { x =>
        val conn = x._1.endpoint.connect()
        conn.select(dbNum)
        getKeyValue(x._2.toSeq, conn)
      }
      .toIterator
      .map(ite => Row.fromSeq(ite))
  }
}

class RedisTableScanUtil(
    val relation: RedisRelationTrait,
    val requiredColumns: Array[String],
    val filters: Map[String, Any])
  extends RedisTableScanTrait {
  override var key: Option[Any] = _
  override var range: Option[(Long, Long)] = _
  override var score: Option[(Double, Double)] = _
  analyzeFilter()
}

trait RedisTableScanTrait {
  val relation: RedisRelationTrait
  val requiredColumns: Array[String]
  val filters: Map[String, Any]
  var key: Option[Any]
  var range: Option[(Long, Long)]
  var score: Option[(Double, Double)]

  lazy val redisType: String = relation.parameters.getOrElse(REDIS_TYPE, "string")
  lazy val tableName: String = relation.parameters.getOrElse(TABLE, "")
  lazy val delimiter: String = relation.parameters.getOrElse(SUFFIX_DELIMITER, ":")
  lazy val dbNum: Int = relation.parameters.getOrElse(DATABASE, "0").toInt

  def getKeyPattern(): String = {
    if (tableName.equals("")) {
      "*"
    } else {
      tableName + delimiter + "*"
    }
  }
  // INLINE: call analyze manual
  def analyzeFilter(): Unit = {
    if (filters.forall(exp => Seq(KEY, SUFFIX, SCORE, RANGE).contains(exp._1))) {
      val keySet = filters.keySet
      key = if (keySet.contains(KEY)) {
        val ink = filters.get(KEY).get.asInstanceOf[Seq[Any]]
        assert(ink.size == 1)
        if (ink.head.isInstanceOf[String]) {
          assert(
            ink.head.asInstanceOf[String].startsWith(tableName),
            "key must start with table name")
        } else {
          assert(
            ink.head.asInstanceOf[Seq[String]].forall(_.startsWith(tableName)),
            "key must start with table name")
        }
        Option(ink.head)
      } else if (keySet.contains(SUFFIX)) {
        val ink = filters.get(SUFFIX).get.asInstanceOf[Seq[Any]]
        assert(ink.size == 1)
        val tmp = ink.head
        Option(if (tmp.isInstanceOf[Seq[String]]) {
          tmp.asInstanceOf[Seq[String]].map(tableName + delimiter + _)
        } else {
          tableName + delimiter + tmp.toString
        })
      } else {
        Option.empty
      }
      range = if (keySet.contains(RANGE)) {
        val ranges = filters.get(RANGE).get.asInstanceOf[Seq[Any]]
        assert(ranges.size == 2)
        Option((ranges(0).toString.toLong, ranges(1).toString.toLong))
      } else {
        Option.empty
      }
      score = if (keySet.contains(SCORE)) {
        val scores = filters.get(SCORE).get.asInstanceOf[Seq[Any]]
        assert(scores.size == 2)
        Option((scores(0).toString.toDouble, scores(1).toString.toDouble))
      } else {
        Option.empty
      }
      if (range.isDefined && Seq("string", "hash", "set").contains(redisType)) {
        throw new SparkException(s"cann't apply range to ${redisType} record")
      }
      if (score.isDefined) {
        if (range.isDefined) {
          throw new SparkException(s"cann't apply range and score at same time")
        }
        if (Seq("string", "hash", "set", "list").contains(redisType)) {
          throw new SparkException(s"cann't apply score to ${redisType} record")
        }
      }
    } else {
      throw new SparkException("only support key, suffix, value, range, score for now")
    }
  }
  def getValuesByBatch(rowkeys: Seq[String], conn: Jedis): Seq[Seq[Any]] = {
    val pipeline = conn.pipelined
    redisType match {
      case "hash" =>
        val noKeyColumns = (requiredColumns.toBuffer - "key").toArray
        rowkeys.foreach(pipeline.hmget(_, noKeyColumns: _*))
        val rows = pipeline
          .syncAndReturnAll()
          .asScala
          .asInstanceOf[Seq[java.util.List[String]]]
          .map(_.asScala)
        val requiredColumnsType = noKeyColumns.map(getDataType(relation.schema, _))
        rows.map(
          row =>
            row
              .zip(requiredColumnsType)
              .map {
                case (col, targetType) =>
                  castToTarget(col, targetType)
            })
      case "string" =>
        rowkeys.foreach(pipeline.get)
        pipeline.syncAndReturnAll().asScala.map(Seq(_))
      case "list" =>
        val (start: Long, end: Long) = if (range.isEmpty) (0L, -1L) else range.get
        rowkeys.foreach(pipeline.lrange(_, start, end))
        pipeline
          .syncAndReturnAll()
          .asScala
          .asInstanceOf[Seq[java.util.List[String]]]
          .map(list => Seq(list.asScala))
      case "set" =>
        rowkeys.foreach(pipeline.smembers)
        pipeline
          .syncAndReturnAll()
          .asScala
          .asInstanceOf[Seq[java.util.Set[String]]]
          .map(set => Seq(set.asScala.toSeq))
      case "zset" =>
        if (score.isDefined) {
          val (min: Double, max: Double) = if (score.isEmpty) {
            (Double.MinValue, Double.MaxValue)
          } else score.get
          rowkeys.foreach(pipeline.zrangeByScoreWithScores(_, min, max))
        } else {
          val (start: Long, end: Long) = if (range.isEmpty) (0L, -1L) else range.get
          rowkeys.foreach(pipeline.zrangeWithScores(_, start, end))
        }
        pipeline
          .syncAndReturnAll()
          .asScala
          .asInstanceOf[Seq[java.util.Set[Tuple]]]
          .map { tups =>
            Seq(
              tups.asScala
                .map {
                  case tup =>
                    (tup.getElement, tup.getScore)
                }
                .toMap[String, Double])
          }
    }
  }

  def getKeyValue(rowkeys: Seq[String], conn: Jedis): Seq[Seq[Any]] = {
    val default_col_name = Seq("key", "value")
    val noKeyColumns = (requiredColumns.toBuffer - "key")
    val toReturn = if (noKeyColumns.size == 0) {
      rowkeys.map(Seq(_))
    } else {
      val col_names = redisType match {
        case "hash" =>
          Seq("key") ++ noKeyColumns
        case "string" | "list" | "set" | "zset" =>
          default_col_name
      }
      val res = rowkeys
        .grouped(10000)
        .flatMap(keys => {
          keys.zip(getValuesByBatch(keys, conn)).map(tup => Seq(tup._1) ++ tup._2)
        })
      val indexs = requiredColumns.map(col_names.indexOf)
      res.filter(_.tail.head != null).map(ite => indexs.map(ite).toSeq).toSeq
    }
    conn.close
    toReturn
  }

  private def getDataType(schema: StructType, attr: String) = {
    schema.fields(schema.fieldIndex(attr)).dataType
  }
  private def castToTarget(value: String, dataType: DataType): Any = {
    dataType match {
      case IntegerType => value.toString.toInt
      case DoubleType => value.toString.toDouble
      case StringType => value.toString
      case _ => value.toString
    }
  }

  /**
   * @param key
   * @return true if the key is a RedisRegex
   */
  private def isRedisRegex(key: String) = {
    def judge(key: String, escape: Boolean): Boolean = {
      if (key.length == 0) {
        false
      } else {
        escape match {
          case true => judge(key.substring(1), false)
          case false =>
            key.charAt(0) match {
              case '*' => true
              case '?' => true
              case '[' => true
              case '\\' => judge(key.substring(1), true)
              case _ => judge(key.substring(1), false)
            }
        }
      }
    }
    judge(key, false)
  }

  def getRowkeys(sPos: Int, ePos: Int): Seq[String] = {
    if (key.isDefined) {
      if (key.get.isInstanceOf[Seq[String]]) {
        key.get.asInstanceOf[Seq[String]].map(_.toString)
      } else {
        Array(key.get.toString).toSeq
      }
    } else {
      getKeys(sPos, ePos, getKeyPattern).asScala.toSeq
    }
  }

  /**
   * @param nodes list of RedisNode
   * @param sPos start position of slots
   * @param ePos end position of slots
   * @param keyPattern
   * return keys whose slot is in [sPos, ePos]
   */
  def getKeys(sPos: Int, ePos: Int, keyPattern: String): util.HashSet[String] = {
    val nodes = relation.redisConfig.getNodesBySlots(sPos, ePos)
    val keys = new util.HashSet[String]()
    if (isRedisRegex(keyPattern)) {
      nodes.foreach(node => {
        val conn = node.endpoint.connect()
        conn.select(dbNum)
        val params = new ScanParams().`match`(keyPattern)
        val res = keys.addAll(
          scanKeys(conn, params).asScala
            .filter(key => {
              val slot = JedisClusterCRC16.getSlot(key)
              slot >= sPos && slot <= ePos
            })
            .asJava)
        conn.close
        res
      })
    } else {
      val slot = JedisClusterCRC16.getSlot(keyPattern)
      if (slot >= sPos && slot <= ePos) keys.add(keyPattern)
    }
    keys
  }

  /**
   * @param nodes list of RedisNode
   * @param keys list of keys
   * return (node: (key1, key2, ...), node2: (key3, key4,...), ...)
   */
  def groupKeysByNode(
      nodes: Array[RedisNode],
      keys: Iterator[String]): Array[(RedisNode, Array[String])] = {
    def getNode(key: String): RedisNode = {
      val slot = JedisClusterCRC16.getSlot(key)
      /* Master only */
      nodes
        .filter(node => { node.startSlot <= slot && node.endSlot >= slot })
        .filter(_.idx == 0)(0)
    }
    keys
      .map(key => (getNode(key), key))
      .toArray
      .groupBy(_._1)
      .map(x => (x._1, x._2.map(_._2)))
      .toArray
  }

  /**
   * @param conn
   * @param keys
   * keys are guaranteed that they belongs with the server jedis connected to.
   * return keys of "t" type
   */
  def filterKeysByType(conn: Jedis, keys: Array[String], t: String): Array[String] = {
    val pipeline = conn.pipelined
    keys.foreach(pipeline.`type`)
    val types = pipeline.syncAndReturnAll
    (keys).zip(types.asScala).filter(x => (x._2 == t)).map(x => x._1)
  }
}
